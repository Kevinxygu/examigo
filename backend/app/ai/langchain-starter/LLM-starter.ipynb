{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read the local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY'] # this is now the API key to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testEnvString = os.environ['TEST_ENV'] # test to see if .env is working\n",
    "print(testEnvString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete direct API call to OpenAI\n",
    "\n",
    "# current model (using GPT 4o)\n",
    "\n",
    "llm_model = 'gpt-4o'\n",
    "\n",
    "# define function to get completion\n",
    "def getCompletion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we would normally call something\n",
    "getCompletion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" However, what if we want to keep on CHAINING these responses on top of each other?\n",
    "If we have several options here, there will be a whole SEQUENCE OF PROMPTS for this.\n",
    "\n",
    "Let's try LangChain out now\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# this creates an object here\n",
    "# when building applications, set temperature 0.0 (0.7 is default)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a prompt here, with a prompt template\n",
    "template_string = template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "# this prompt_template, based on the one from above, detects the two input_variables style and text.\n",
    "prompt_template\n",
    "\n",
    "# the input variables are all here\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style=\"\"\"In a very professional, succinct, and business-forward manner. Should commnunicate information clearly\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Walkin' with my back to the sun, keep my head to the sky \\\n",
    "Me against the world, it's me, myself and I, like De La \\\n",
    "Got in touch with my soul \\\n",
    "Tradin' softly on a path down the rockiest road \\\n",
    "Life isn't ice cream without Monopoly dough \\\n",
    "The property grows in value, and rightfully so, I gotta have it\n",
    "\"\"\"\n",
    "\n",
    "# this will generate a prompt\n",
    "messages = prompt_template.format_messages(style=style,\n",
    "                                           text=text)\n",
    "# this will generate a prompt\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat!\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above is an application for LangChain. How to create strong and flexible prompts to feed into the LLM + model\n",
    "# Prompt templates are useful for sophisticated applications. Good to re-use prompts that are good\n",
    "\n",
    "# langchain prompt library also utilizes output prompting. however, you can help LLM to return a specific output\n",
    "# (e.g. chain-of-thought reasoning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
